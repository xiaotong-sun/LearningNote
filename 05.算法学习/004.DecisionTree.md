# 决策树CART算法

介绍算法的细节，包括其假设、学习准则、优化算法、模型预测。



## 1.概述

决策树是一种常见的机器学习算法，用于从输入数据中推断出关于数据特征的结论。通过类似树状结构的决策图，它能够帮助我们做出决策或预测结果。在每个内部节点，决策树根据输入数据的特征进行划分；在叶子节点，它给出了数据的预测结果。决策树非常直观易懂，同时也可以处理分类和回归任务。在训练过程中，决策树算法会根据数据特征不断进行分裂，直到达到预先设定的停止条件。它也可以处理缺失数据，并且对异常值具有一定的鲁棒性。决策树在实际应用中具有很好的可解释性，能够帮助分析人员理解模型的决策过程，因此被广泛应用于各行各业的数据分析和决策支持中。 

决策树的生成算法有ID3，C4.5和CART等。ID3和C4.5算法生成的决策树是多叉树，只能处理分类而不能处理回归。而CART算法（即分类回归树算法）生成的算法是一棵二叉树，既可以用于分类也可以用于回归，分类树的输出是样本的类别，回归树的输出是一个实数。本文重点介绍CART算法。

在CART算法中，对于回归树，采用的是**平方误差最小化准则**；对于分类树，采用**基尼系数最小化准则**。

CART算法的步骤包括以下几点：

1. 特征选择；
2. 递归建立决策树；
3. 决策树剪枝；



## 2.CART分类树算法

ID3中使用了**信息增益**选择特征，增益大的优先选择。在C4.5算法中，采用**信息增益率**来选择特征，减少因特征值多导致信息增益大的问题。CART分类树算法使用**基尼系数**选择特征，基尼系数代表了模型的不纯度，**基尼系数越小，不纯度越低，特征越好**。这和信息增益(率)相反。



### 2.1 基尼系数

在分类问题中，假设数据集D有K个类别，第k个类别的概率为$p_k$, 则基尼系数的表达式为：
$$
Gini(D)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
$$
如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：
$$
Gini(D)=2p(1-p)
$$
从直观上来说，数据集的基尼系数反映了从数据集D中随机抽取两个样本，其类别不一样的概率。因此，，基尼系数越小，数据集的纯度就越高。

对于个给定的样本D,假设有K个类别, 第k个类别的数量为$C_k$,则样本D的基尼系数表达式为：
$$
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$
特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$
为了简化基尼系数的计算，CART分类树算法每次仅对某个特征值进行二分，而不是多分，这样CART分类树算法建立起来的就是一棵二叉树，而不是多叉树。

